definitions:
  main.InferBatchStatistics:
    properties:
      batch_size:
        description: |-
          @@ .. cpp:var:: uint64 batch_size
          @@
          @@ The size of the batch.
          @@
        type: integer
      compute_infer:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration compute_infer
          @@
          @@ The count and cumulative duration to execute the model with the given
          @@ batch size.
          @@
      compute_input:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration compute_input
          @@
          @@ The count and cumulative duration to prepare input tensor data as
          @@ required by the model framework / backend with the given batch size.
          @@ For example, this duration should include the time to copy input
          @@ tensor data to the GPU.
          @@
      compute_output:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration compute_output
          @@
          @@ The count and cumulative duration to extract output tensor data
          @@ produced by the model framework / backend with the given batch size.
          @@ For example, this duration should include the time to copy output
          @@ tensor data from the GPU.
          @@
    type: object
  main.InferStatistics:
    properties:
      cache_hit:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration cache_hit
          @@
          @@ The count of response cache hits and cumulative duration to lookup
          @@ and extract output tensor data from the Response Cache on a cache
          @@ hit. For example, this duration should include the time to copy
          @@ output tensor data from the Response Cache to the response object.
          @@ On cache hits, triton does not need to go to the model/backend
          @@ for the output tensor data, so the "compute_input", "compute_infer",
          @@ and "compute_output" fields are not updated. Assuming the response
          @@ cache is enabled for a given model, a cache hit occurs for a
          @@ request to that model when the request metadata (model name,
          @@ model version, model inputs) hashes to an existing entry in the
          @@ cache. On a cache miss, the request hash and response output tensor
          @@ data is added to the cache. See response cache docs for more info:
          @@ https://github.com/triton-inference-server/server/blob/main/docs/response_cache.md
          @@
      cache_miss:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration cache_miss
          @@
          @@ The count of response cache misses and cumulative duration to lookup
          @@ and insert output tensor data from the computed response to the cache.
          @@ For example, this duration should include the time to copy
          @@ output tensor data from the response object to the Response Cache.
          @@ Assuming the response cache is enabled for a given model, a cache
          @@ miss occurs for a request to that model when the request metadata
          @@ does NOT hash to an existing entry in the cache. See the response
          @@ cache docs for more info:
          @@ https://github.com/triton-inference-server/server/blob/main/docs/response_cache.md
          @@
      compute_infer:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration compute_infer
          @@
          @@ The count and cumulative duration to execute the model.
          @@ The "compute_infer" count and cumulative duration do not account for
          @@ requests that were a cache hit. See the "cache_hit" field for more
          @@ info.
          @@
      compute_input:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration compute_input
          @@
          @@ The count and cumulative duration to prepare input tensor data as
          @@ required by the model framework / backend. For example, this duration
          @@ should include the time to copy input tensor data to the GPU.
          @@ The "compute_input" count and cumulative duration do not account for
          @@ requests that were a cache hit. See the "cache_hit" field for more
          @@ info.
          @@
      compute_output:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration compute_output
          @@
          @@ The count and cumulative duration to extract output tensor data
          @@ produced by the model framework / backend. For example, this duration
          @@ should include the time to copy output tensor data from the GPU.
          @@ The "compute_output" count and cumulative duration do not account for
          @@ requests that were a cache hit. See the "cache_hit" field for more
          @@ info.
          @@
      fail:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration fail
          @@
          @@ Cumulative count and duration for failed inference
          @@ request.
          @@
      queue:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration queue
          @@
          @@ The count and cumulative duration that inference requests wait in
          @@ scheduling or other queues. The "queue" count and cumulative
          @@ duration includes cache hits.
          @@
      success:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration success
          @@
          @@ Cumulative count and duration for successful inference
          @@ request. The "success" count and cumulative duration includes
          @@ cache hits.
          @@
    type: object
  main.ModelMetadataResponse:
    properties:
      inputs:
        description: |-
          @@
          @@ .. cpp:var:: TensorMetadata inputs (repeated)
          @@
          @@ The model's inputs.
          @@
        items:
          $ref: '#/definitions/main.ModelMetadataResponse_TensorMetadata'
        type: array
      name:
        description: |-
          @@
          @@ .. cpp:var:: string name
          @@
          @@ The model name.
          @@
        type: string
      outputs:
        description: |-
          @@
          @@ .. cpp:var:: TensorMetadata outputs (repeated)
          @@
          @@ The model's outputs.
          @@
        items:
          $ref: '#/definitions/main.ModelMetadataResponse_TensorMetadata'
        type: array
      platform:
        description: |-
          @@
          @@ .. cpp:var:: string platform
          @@
          @@ The model's platform.
          @@
        type: string
      versions:
        description: |-
          @@
          @@ .. cpp:var:: string versions (repeated)
          @@
          @@ The versions of the model.
          @@
        items:
          type: string
        type: array
    type: object
  main.ModelMetadataResponse_TensorMetadata:
    properties:
      datatype:
        description: |-
          @@
          @@ .. cpp:var:: string datatype
          @@
          @@ The tensor data type.
          @@
        type: string
      name:
        description: |-
          @@
          @@ .. cpp:var:: string name
          @@
          @@ The tensor name.
          @@
        type: string
      shape:
        description: |-
          @@
          @@ .. cpp:var:: int64 shape (repeated)
          @@
          @@ The tensor shape. A variable-size dimension is represented
          @@ by a -1 value.
          @@
        items:
          type: integer
        type: array
    type: object
  main.ModelStatistics:
    properties:
      batch_stats:
        description: |-
          @@ .. cpp:var:: InferBatchStatistics batch_stats (repeated)
          @@
          @@ The aggregate statistics for each different batch size that is
          @@ executed in the model. The batch statistics indicate how many actual
          @@ model executions were performed and show differences due to different
          @@ batch size (for example, larger batches typically take longer to
          @@ compute).
          @@
        items:
          $ref: '#/definitions/main.InferBatchStatistics'
        type: array
      execution_count:
        description: |-
          @@ .. cpp:var:: uint64 last_inference
          @@
          @@ The cumulative count of the number of successful inference executions
          @@ performed for the model. When dynamic batching is enabled, a single
          @@ model execution can perform inferencing for more than one inference
          @@ request. For example, if a clients sends 64 individual requests each
          @@ with batch size 1 and the dynamic batcher batches them into a single
          @@ large batch for model execution then "execution_count" will be
          @@ incremented by 1. If, on the other hand, the dynamic batcher is not
          @@ enabled for that each of the 64 individual requests is executed
          @@ independently, then "execution_count" will be incremented by 64.
          @@ The "execution_count" value DOES NOT include cache hits.
          @@
        type: integer
      inference_count:
        description: |-
          @@ .. cpp:var:: uint64 last_inference
          @@
          @@ The cumulative count of successful inference requests made for this
          @@ model. Each inference in a batched request is counted as an
          @@ individual inference. For example, if a client sends a single
          @@ inference request with batch size 64, "inference_count" will be
          @@ incremented by 64. Similarly, if a clients sends 64 individual
          @@ requests each with batch size 1, "inference_count" will be
          @@ incremented by 64. The "inference_count" value DOES NOT include
          @@ cache hits.
          @@
        type: integer
      inference_stats:
        $ref: '#/definitions/main.InferStatistics'
        description: |-
          @@ .. cpp:var:: InferStatistics inference_stats
          @@
          @@ The aggregate statistics for the model/version.
          @@
      last_inference:
        description: |-
          @@ .. cpp:var:: uint64 last_inference
          @@
          @@ The timestamp of the last inference request made for this model,
          @@ as milliseconds since the epoch.
          @@
        type: integer
      name:
        description: |-
          @@ .. cpp:var:: string name
          @@
          @@ The name of the model. If not given returns statistics for all
          @@
        type: string
      version:
        description: |-
          @@ .. cpp:var:: string version
          @@
          @@ The version of the model.
          @@
        type: string
    type: object
  main.ModelStatisticsResponse:
    properties:
      model_stats:
        description: |-
          @@ .. cpp:var:: ModelStatistics model_stats (repeated)
          @@
          @@ Statistics for each requested model.
          @@
        items:
          $ref: '#/definitions/main.ModelStatistics'
        type: array
    type: object
  main.StatisticDuration:
    properties:
      count:
        description: |-
          @@ .. cpp:var:: uint64 count
          @@
          @@ Cumulative number of times this metric occurred.
          @@
        type: integer
      ns:
        description: |-
          @@ .. cpp:var:: uint64 total_time_ns
          @@
          @@ Total collected duration of this metric in nanoseconds.
          @@
        type: integer
    type: object
info:
  contact:
    email: www.jwpark.co.kr@gmail.com
    name: Curt-Park
paths:
  /:
    get:
      consumes:
      - application/json
      description: It returns true if the api server is alive
      produces:
      - application/json
      responses:
        "200":
          description: API server's liveness
          schema:
            type: boolean
      summary: Healthcheck
  /liveness:
    get:
      consumes:
      - application/json
      description: It returns true if the triton server is alive
      produces:
      - application/json
      responses:
        "200":
          description: Triton server's liveness
          schema:
            type: boolean
      summary: Check Triton's liveness
  /model-metadata:
    get:
      consumes:
      - application/json
      description: It returns the requested model metadata
      parameters:
      - description: model name
        in: query
        name: model
        required: true
        type: string
      - description: model version
        in: query
        name: version
        type: string
      produces:
      - application/json
      responses:
        "200":
          description: Triton server's model metadata
          schema:
            $ref: '#/definitions/main.ModelMetadataResponse'
      summary: Get model metadata
  /model-stats:
    get:
      consumes:
      - application/json
      description: It returns the requested model's inference statistics
      parameters:
      - description: model name
        in: query
        name: model
        required: true
        type: string
      - description: model version
        in: query
        name: version
        type: string
      produces:
      - application/json
      responses:
        "200":
          description: Triton server's model statistics
          schema:
            $ref: '#/definitions/main.ModelStatisticsResponse'
      summary: Get model inference statistics
  /readiness:
    get:
      consumes:
      - application/json
      description: It returns true if the triton server is ready
      produces:
      - application/json
      responses:
        "200":
          description: Triton server's readiness
          schema:
            type: boolean
      summary: Check Triton's Readiness
swagger: "2.0"
