definitions:
  main.InferBatchStatistics:
    properties:
      batch_size:
        description: |-
          @@ .. cpp:var:: uint64 batch_size
          @@
          @@ The size of the batch.
          @@
        type: integer
      compute_infer:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration compute_infer
          @@
          @@ The count and cumulative duration to execute the model with the given
          @@ batch size.
          @@
      compute_input:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration compute_input
          @@
          @@ The count and cumulative duration to prepare input tensor data as
          @@ required by the model framework / backend with the given batch size.
          @@ For example, this duration should include the time to copy input
          @@ tensor data to the GPU.
          @@
      compute_output:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration compute_output
          @@
          @@ The count and cumulative duration to extract output tensor data
          @@ produced by the model framework / backend with the given batch size.
          @@ For example, this duration should include the time to copy output
          @@ tensor data from the GPU.
          @@
    type: object
  main.InferParameter:
    properties:
      parameterChoice:
        description: "@@ .. cpp:var:: oneof parameter_choice\n@@\n@@ The parameter
          value can be a string, an int64 or\n@@ a boolean\n@@\n\nTypes that are assignable
          to ParameterChoice:\n\n\t*InferParameter_BoolParam\n\t*InferParameter_Int64Param\n\t*InferParameter_StringParam"
    type: object
  main.InferStatistics:
    properties:
      cache_hit:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration cache_hit
          @@
          @@ The count of response cache hits and cumulative duration to lookup
          @@ and extract output tensor data from the Response Cache on a cache
          @@ hit. For example, this duration should include the time to copy
          @@ output tensor data from the Response Cache to the response object.
          @@ On cache hits, triton does not need to go to the model/backend
          @@ for the output tensor data, so the "compute_input", "compute_infer",
          @@ and "compute_output" fields are not updated. Assuming the response
          @@ cache is enabled for a given model, a cache hit occurs for a
          @@ request to that model when the request metadata (model name,
          @@ model version, model inputs) hashes to an existing entry in the
          @@ cache. On a cache miss, the request hash and response output tensor
          @@ data is added to the cache. See response cache docs for more info:
          @@ https://github.com/triton-inference-server/server/blob/main/docs/response_cache.md
          @@
      cache_miss:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration cache_miss
          @@
          @@ The count of response cache misses and cumulative duration to lookup
          @@ and insert output tensor data from the computed response to the cache.
          @@ For example, this duration should include the time to copy
          @@ output tensor data from the response object to the Response Cache.
          @@ Assuming the response cache is enabled for a given model, a cache
          @@ miss occurs for a request to that model when the request metadata
          @@ does NOT hash to an existing entry in the cache. See the response
          @@ cache docs for more info:
          @@ https://github.com/triton-inference-server/server/blob/main/docs/response_cache.md
          @@
      compute_infer:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration compute_infer
          @@
          @@ The count and cumulative duration to execute the model.
          @@ The "compute_infer" count and cumulative duration do not account for
          @@ requests that were a cache hit. See the "cache_hit" field for more
          @@ info.
          @@
      compute_input:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration compute_input
          @@
          @@ The count and cumulative duration to prepare input tensor data as
          @@ required by the model framework / backend. For example, this duration
          @@ should include the time to copy input tensor data to the GPU.
          @@ The "compute_input" count and cumulative duration do not account for
          @@ requests that were a cache hit. See the "cache_hit" field for more
          @@ info.
          @@
      compute_output:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration compute_output
          @@
          @@ The count and cumulative duration to extract output tensor data
          @@ produced by the model framework / backend. For example, this duration
          @@ should include the time to copy output tensor data from the GPU.
          @@ The "compute_output" count and cumulative duration do not account for
          @@ requests that were a cache hit. See the "cache_hit" field for more
          @@ info.
          @@
      fail:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration fail
          @@
          @@ Cumulative count and duration for failed inference
          @@ request.
          @@
      queue:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration queue
          @@
          @@ The count and cumulative duration that inference requests wait in
          @@ scheduling or other queues. The "queue" count and cumulative
          @@ duration includes cache hits.
          @@
      success:
        $ref: '#/definitions/main.StatisticDuration'
        description: |-
          @@ .. cpp:var:: StatisticDuration success
          @@
          @@ Cumulative count and duration for successful inference
          @@ request. The "success" count and cumulative duration includes
          @@ cache hits.
          @@
    type: object
  main.InferTensorContents:
    properties:
      bool_contents:
        description: |-
          @@
          @@ .. cpp:var:: bool bool_contents (repeated)
          @@
          @@ Representation for BOOL data type. The size must match what is
          @@ expected by the tensor's shape. The contents must be the flattened,
          @@ one-dimensional, row-major order of the tensor elements.
          @@
        items:
          type: boolean
        type: array
      bytes_contents:
        description: |-
          @@
          @@ .. cpp:var:: bytes bytes_contents (repeated)
          @@
          @@ Representation for BYTES data type. The size must match what is
          @@ expected by the tensor's shape. The contents must be the flattened,
          @@ one-dimensional, row-major order of the tensor elements.
          @@
        items:
          items:
            type: integer
          type: array
        type: array
      fp32_contents:
        description: |-
          @@
          @@ .. cpp:var:: float fp32_contents (repeated)
          @@
          @@ Representation for FP32 data type. The size must match what is
          @@ expected by the tensor's shape. The contents must be the flattened,
          @@ one-dimensional, row-major order of the tensor elements.
          @@
        items:
          type: number
        type: array
      fp64_contents:
        description: |-
          @@
          @@ .. cpp:var:: double fp64_contents (repeated)
          @@
          @@ Representation for FP64 data type. The size must match what is
          @@ expected by the tensor's shape. The contents must be the flattened,
          @@ one-dimensional, row-major order of the tensor elements.
          @@
        items:
          type: number
        type: array
      int_contents:
        description: |-
          @@
          @@ .. cpp:var:: int32 int_contents (repeated)
          @@
          @@ Representation for INT8, INT16, and INT32 data types. The size
          @@ must match what is expected by the tensor's shape. The contents
          @@ must be the flattened, one-dimensional, row-major order of the
          @@ tensor elements.
          @@
        items:
          type: integer
        type: array
      int64_contents:
        description: |-
          @@
          @@ .. cpp:var:: int64 int64_contents (repeated)
          @@
          @@ Representation for INT64 data types. The size must match what
          @@ is expected by the tensor's shape. The contents must be the
          @@ flattened, one-dimensional, row-major order of the tensor elements.
          @@
        items:
          type: integer
        type: array
      uint_contents:
        description: |-
          @@
          @@ .. cpp:var:: uint32 uint_contents (repeated)
          @@
          @@ Representation for UINT8, UINT16, and UINT32 data types. The size
          @@ must match what is expected by the tensor's shape. The contents
          @@ must be the flattened, one-dimensional, row-major order of the
          @@ tensor elements.
          @@
        items:
          type: integer
        type: array
      uint64_contents:
        description: |-
          @@
          @@ .. cpp:var:: uint64 uint64_contents (repeated)
          @@
          @@ Representation for UINT64 data types. The size must match what
          @@ is expected by the tensor's shape. The contents must be the
          @@ flattened, one-dimensional, row-major order of the tensor elements.
          @@
        items:
          type: integer
        type: array
    type: object
  main.ModelInferResponse:
    properties:
      id:
        description: |-
          @@ .. cpp:var:: string id
          @@
          @@ The id of the inference request if one was specified.
          @@
        type: string
      model_name:
        description: |-
          @@ .. cpp:var:: string model_name
          @@
          @@ The name of the model used for inference.
          @@
        type: string
      model_version:
        description: |-
          @@ .. cpp:var:: string model_version
          @@
          @@ The version of the model used for inference.
          @@
        type: string
      outputs:
        description: |-
          @@
          @@ .. cpp:var:: InferOutputTensor outputs (repeated)
          @@
          @@ The output tensors holding inference results.
          @@
        items:
          $ref: '#/definitions/main.ModelInferResponse_InferOutputTensor'
        type: array
      parameters:
        additionalProperties:
          $ref: '#/definitions/main.InferParameter'
        description: |-
          @@ .. cpp:var:: map<string,InferParameter> parameters
          @@
          @@ Optional inference response parameters.
          @@
        type: object
      raw_output_contents:
        description: |-
          @@
          @@ .. cpp:var:: bytes raw_output_contents
          @@
          @@ The data contained in an output tensor can be represented in
          @@ "raw" bytes form or in the repeated type that matches the
          @@ tensor's data type. Using the "raw" bytes form will
          @@ typically allow higher performance due to the way protobuf
          @@ allocation and reuse interacts with GRPC. For example, see
          @@ https://github.com/grpc/grpc/issues/23231.
          @@
          @@ To use the raw representation 'raw_output_contents' must be
          @@ initialized with data for each tensor in the same order as
          @@ 'outputs'. For each tensor, the size of this content must
          @@ match what is expected by the tensor's shape and data
          @@ type. The raw data must be the flattened, one-dimensional,
          @@ row-major order of the tensor elements without any stride
          @@ or padding between the elements. Note that the FP16 and BF16 data
          @@ types must be represented as raw content as there is no
          @@ specific data type for a 16-bit float type.
          @@
          @@ If this field is specified then InferOutputTensor::contents
          @@ must not be specified for any output tensor.
          @@
        items:
          items:
            type: integer
          type: array
        type: array
    type: object
  main.ModelInferResponse_InferOutputTensor:
    properties:
      contents:
        $ref: '#/definitions/main.InferTensorContents'
        description: |-
          @@ .. cpp:var:: InferTensorContents contents
          @@
          @@ The tensor contents using a data-type format. This field
          @@ must not be specified if tensor contents are being specified
          @@ in ModelInferResponse.raw_output_contents.
          @@
      datatype:
        description: |-
          @@
          @@ .. cpp:var:: string datatype
          @@
          @@ The tensor data type.
          @@
        type: string
      name:
        description: |-
          @@
          @@ .. cpp:var:: string name
          @@
          @@ The tensor name.
          @@
        type: string
      parameters:
        additionalProperties:
          $ref: '#/definitions/main.InferParameter'
        description: |-
          @@ .. cpp:var:: map<string,InferParameter> parameters
          @@
          @@ Optional output tensor parameters.
          @@
        type: object
      shape:
        description: |-
          @@
          @@ .. cpp:var:: int64 shape (repeated)
          @@
          @@ The tensor shape.
          @@
        items:
          type: integer
        type: array
    type: object
  main.ModelMetadataResponse:
    properties:
      inputs:
        description: |-
          @@
          @@ .. cpp:var:: TensorMetadata inputs (repeated)
          @@
          @@ The model's inputs.
          @@
        items:
          $ref: '#/definitions/main.ModelMetadataResponse_TensorMetadata'
        type: array
      name:
        description: |-
          @@
          @@ .. cpp:var:: string name
          @@
          @@ The model name.
          @@
        type: string
      outputs:
        description: |-
          @@
          @@ .. cpp:var:: TensorMetadata outputs (repeated)
          @@
          @@ The model's outputs.
          @@
        items:
          $ref: '#/definitions/main.ModelMetadataResponse_TensorMetadata'
        type: array
      platform:
        description: |-
          @@
          @@ .. cpp:var:: string platform
          @@
          @@ The model's platform.
          @@
        type: string
      versions:
        description: |-
          @@
          @@ .. cpp:var:: string versions (repeated)
          @@
          @@ The versions of the model.
          @@
        items:
          type: string
        type: array
    type: object
  main.ModelMetadataResponse_TensorMetadata:
    properties:
      datatype:
        description: |-
          @@
          @@ .. cpp:var:: string datatype
          @@
          @@ The tensor data type.
          @@
        type: string
      name:
        description: |-
          @@
          @@ .. cpp:var:: string name
          @@
          @@ The tensor name.
          @@
        type: string
      shape:
        description: |-
          @@
          @@ .. cpp:var:: int64 shape (repeated)
          @@
          @@ The tensor shape. A variable-size dimension is represented
          @@ by a -1 value.
          @@
        items:
          type: integer
        type: array
    type: object
  main.ModelStatistics:
    properties:
      batch_stats:
        description: |-
          @@ .. cpp:var:: InferBatchStatistics batch_stats (repeated)
          @@
          @@ The aggregate statistics for each different batch size that is
          @@ executed in the model. The batch statistics indicate how many actual
          @@ model executions were performed and show differences due to different
          @@ batch size (for example, larger batches typically take longer to
          @@ compute).
          @@
        items:
          $ref: '#/definitions/main.InferBatchStatistics'
        type: array
      execution_count:
        description: |-
          @@ .. cpp:var:: uint64 last_inference
          @@
          @@ The cumulative count of the number of successful inference executions
          @@ performed for the model. When dynamic batching is enabled, a single
          @@ model execution can perform inferencing for more than one inference
          @@ request. For example, if a clients sends 64 individual requests each
          @@ with batch size 1 and the dynamic batcher batches them into a single
          @@ large batch for model execution then "execution_count" will be
          @@ incremented by 1. If, on the other hand, the dynamic batcher is not
          @@ enabled for that each of the 64 individual requests is executed
          @@ independently, then "execution_count" will be incremented by 64.
          @@ The "execution_count" value DOES NOT include cache hits.
          @@
        type: integer
      inference_count:
        description: |-
          @@ .. cpp:var:: uint64 last_inference
          @@
          @@ The cumulative count of successful inference requests made for this
          @@ model. Each inference in a batched request is counted as an
          @@ individual inference. For example, if a client sends a single
          @@ inference request with batch size 64, "inference_count" will be
          @@ incremented by 64. Similarly, if a clients sends 64 individual
          @@ requests each with batch size 1, "inference_count" will be
          @@ incremented by 64. The "inference_count" value DOES NOT include
          @@ cache hits.
          @@
        type: integer
      inference_stats:
        $ref: '#/definitions/main.InferStatistics'
        description: |-
          @@ .. cpp:var:: InferStatistics inference_stats
          @@
          @@ The aggregate statistics for the model/version.
          @@
      last_inference:
        description: |-
          @@ .. cpp:var:: uint64 last_inference
          @@
          @@ The timestamp of the last inference request made for this model,
          @@ as milliseconds since the epoch.
          @@
        type: integer
      name:
        description: |-
          @@ .. cpp:var:: string name
          @@
          @@ The name of the model. If not given returns statistics for all
          @@
        type: string
      version:
        description: |-
          @@ .. cpp:var:: string version
          @@
          @@ The version of the model.
          @@
        type: string
    type: object
  main.ModelStatisticsResponse:
    properties:
      model_stats:
        description: |-
          @@ .. cpp:var:: ModelStatistics model_stats (repeated)
          @@
          @@ Statistics for each requested model.
          @@
        items:
          $ref: '#/definitions/main.ModelStatistics'
        type: array
    type: object
  main.RepositoryModelLoadResponse:
    type: object
  main.RepositoryModelUnloadResponse:
    type: object
  main.StatisticDuration:
    properties:
      count:
        description: |-
          @@ .. cpp:var:: uint64 count
          @@
          @@ Cumulative number of times this metric occurred.
          @@
        type: integer
      ns:
        description: |-
          @@ .. cpp:var:: uint64 total_time_ns
          @@
          @@ Total collected duration of this metric in nanoseconds.
          @@
        type: integer
    type: object
info:
  contact:
    email: www.jwpark.co.kr@gmail.com
    name: Curt-Park
paths:
  /:
    get:
      consumes:
      - application/json
      description: It returns true if the api server is alive.
      produces:
      - application/json
      responses:
        "200":
          description: API server's liveness
          schema:
            type: boolean
      summary: Healthcheck
  /liveness:
    get:
      consumes:
      - application/json
      description: It returns true if the triton server is alive.
      produces:
      - application/json
      responses:
        "200":
          description: Triton server's liveness
          schema:
            type: boolean
      summary: Check Triton's liveness.
  /model-load:
    post:
      consumes:
      - application/json
      description: It requests to load a model. This is only allowed when polling
        is enabled.
      parameters:
      - description: model name
        in: query
        name: model
        required: true
        type: string
      produces:
      - application/json
      responses:
        "200":
          description: Triton server's model load response
          schema:
            $ref: '#/definitions/main.RepositoryModelLoadResponse'
      summary: Load a model.
  /model-metadata:
    get:
      consumes:
      - application/json
      description: It returns the requested model metadata
      parameters:
      - description: model name
        in: query
        name: model
        required: true
        type: string
      - description: model version
        in: query
        name: version
        type: string
      produces:
      - application/json
      responses:
        "200":
          description: Triton server's model metadata
          schema:
            $ref: '#/definitions/main.ModelMetadataResponse'
      summary: Get model metadata.
  /model-stats:
    get:
      consumes:
      - application/json
      description: It returns the requested model's inference statistics.
      parameters:
      - description: model name
        in: query
        name: model
        required: true
        type: string
      - description: model version
        in: query
        name: version
        type: string
      produces:
      - application/json
      responses:
        "200":
          description: Triton server's model statistics
          schema:
            $ref: '#/definitions/main.ModelStatisticsResponse'
      summary: Get model inference statistics.
  /model-unload:
    post:
      consumes:
      - application/json
      description: It requests to unload a model. This is only allowed when polling
        is enabled.
      parameters:
      - description: model name
        in: query
        name: model
        required: true
        type: string
      produces:
      - application/json
      responses:
        "200":
          description: Triton server's model unload response
          schema:
            $ref: '#/definitions/main.RepositoryModelUnloadResponse'
      summary: Unload a model.
  /readiness:
    get:
      consumes:
      - application/json
      description: It returns true if the triton server is ready.
      produces:
      - application/json
      responses:
        "200":
          description: Triton server's readiness
          schema:
            type: boolean
      summary: Check Triton's Readiness.
  /simple-infer:
    post:
      consumes:
      - application/json
      description: It outputs a single bytes with a single bytes input.
      parameters:
      - description: model name
        in: formData
        name: model
        required: true
        type: string
      - description: input
        in: formData
        name: file
        required: true
        type: file
      - description: model version
        in: formData
        name: version
        type: string
      produces:
      - application/json
      responses:
        "200":
          description: Triton server's inference response
          schema:
            $ref: '#/definitions/main.ModelInferResponse'
      summary: model inference api for the model with bytes a input and a bytes output.
swagger: "2.0"
