package main

import (
	"context"
	_ "echo-grpc-triton/docs" // docs is generated by Swag CLI, you have to import it.
	"flag"
	"github.com/labstack/echo/v4"
	"github.com/labstack/echo/v4/middleware"
	"github.com/swaggo/echo-swagger"
	"google.golang.org/grpc"
	"io/ioutil"
	"log"
	"net/http" // Package http provides HTTP client and server implementations.
	"time"
)

// Flags contains the information to send requests to Triton inference server.
type Flags struct {
	ModelName    string
	ModelVersion string
	URL          string
	TIMEOUT      int64
}

// Contexts that is necessary to communicate with Triton
type Client struct {
	grpc GRPCInferenceServiceClient
}

// Global variables
var flags = Flags{}
var client = Client{}

// parseFlags parses the arguments and initialize the flags.
func (flags *Flags) parseFlags() {
	// https://github.com/NVIDIA/triton-inference-server/tree/master/docs/examples/model_repository/simple
	flag.StringVar(&flags.URL, "u", "localhost:8001", "Inference Server URL. Default: localhost:8001")
	flag.Int64Var(&flags.TIMEOUT, "t", 10, "Timeout. Default: 10 Sec.")
	flag.Parse()
}

// ConnectToTritonWithGRPC Create GRPC Connection
func (client *Client) connectToTriton(url string) {
	conn, err := grpc.Dial(url, grpc.WithInsecure())
	if err != nil {
		log.Fatalf("couldn't connect to endpoint %s: %v", url, err)
	}
	client.grpc = NewGRPCInferenceServiceClient(conn)
}

// @contact.name  Curt-Park
// @contact.email www.jwpark.co.kr@gmail.com
func main() {
	// Parse the args
	flags.parseFlags()
	log.Println("Flags:", flags)

	// Check the gRPC connection well-established
	client.connectToTriton(flags.URL)

	// Create a server with echo
	e := echo.New()
	// Logger middleware logs the information about each HTTP request
	e.Use(middleware.Logger())

	// APIs
	e.GET("/", getHealthCheck)
	e.GET("/liveness", getServerLiveness)
	e.GET("/readiness", getServerLiveness)
	e.GET("/model-metadata", getModelMetadata)
	e.GET("/model-stats", getModelInferStats)
	e.POST("/model-load", loadModel)
	e.POST("/model-unload", unloadModel)
	e.POST("/infer", infer)

	// Swagger
	e.GET("/docs/*", echoSwagger.WrapHandler)
	e.Logger.Fatal(e.Start(":8080"))
}

// @Summary     Healthcheck
// @Description It returns true if the api server is alive.
// @Accept      json
// @Produce     json
// @Success     200 {object} bool "API server's liveness"
// @Router      / [get]
func getHealthCheck(c echo.Context) error {
	return c.JSON(http.StatusOK, true)
}

// @Summary     Check Triton's liveness.
// @Description It returns true if the triton server is alive.
// @Accept      json
// @Produce     json
// @Success     200 {object} bool "Triton server's liveness"
// @Router      /liveness [get]
func getServerLiveness(c echo.Context) error {
	ctx, cancel := context.WithTimeout(context.Background(), time.Duration(flags.TIMEOUT)*time.Second)
	defer cancel()

	serverLiveRequest := ServerLiveRequest{}
	serverLiveResponse, err := client.grpc.ServerLive(ctx, &serverLiveRequest)
	if err != nil {
		return err
	}
	return c.JSON(http.StatusOK, serverLiveResponse.Live)
}

// @Summary     Check Triton's Readiness.
// @Description It returns true if the triton server is ready.
// @Accept      json
// @Produce     json
// @Success     200 {object} bool "Triton server's readiness"
// @Router      /readiness [get]
func getServerReadiness(c echo.Context) error {
	ctx, cancel := context.WithTimeout(context.Background(), time.Duration(flags.TIMEOUT)*time.Second)
	defer cancel()

	serverReadyRequest := ServerReadyRequest{}
	serverReadyResponse, err := client.grpc.ServerReady(ctx, &serverReadyRequest)
	if err != nil {
		return err
	}
	return c.JSON(http.StatusOK, serverReadyResponse.Ready)
}

// @Summary     Get model metadata.
// @Description It returns the requested model metadata
// @Accept      json
// @Produce     json
// @Param       model   query    string                true  "model name"
// @Param       version query    string                false "model version"
// @Success     200     {object} ModelMetadataResponse "Triton server's model metadata"
// @Router      /model-metadata [get]
func getModelMetadata(c echo.Context) error {
	ctx, cancel := context.WithTimeout(context.Background(), time.Duration(flags.TIMEOUT)*time.Second)
	defer cancel()

	modelMetadataRequest := ModelMetadataRequest{Name: c.QueryParam("model"), Version: c.QueryParam("version")}
	modelMetadataResponse, err := client.grpc.ModelMetadata(ctx, &modelMetadataRequest)
	if err != nil {
		return err
	}
	return c.JSON(http.StatusOK, modelMetadataResponse)
}

// @Summary     Get model inference statistics.
// @Description It returns the requested model's inference statistics.
// @Accept      json
// @Produce     json
// @Param       model   query    string                  true  "model name"
// @Param       version query    string                  false "model version"
// @Success     200     {object} ModelStatisticsResponse "Triton server's model statistics"
// @Router      /model-stats [get]
func getModelInferStats(c echo.Context) error {
	ctx, cancel := context.WithTimeout(context.Background(), time.Duration(flags.TIMEOUT)*time.Second)
	defer cancel()

	modelStatisticsRequest := ModelStatisticsRequest{Name: c.QueryParam("model"), Version: c.QueryParam("version")}
	modelStatisticsResponse, err := client.grpc.ModelStatistics(ctx, &modelStatisticsRequest)
	if err != nil {
		return err
	}
	return c.JSON(http.StatusOK, modelStatisticsResponse)
}

// @Summary     Load a model.
// @Description It requests to load a model. This is only allowed when polling is enabled.
// @Accept      json
// @Produce     json
// @Param       model query    string                      true "model name"
// @Success     200   {object} RepositoryModelLoadResponse "Triton server's model load response"
// @Router      /model-load [post]
func loadModel(c echo.Context) error {
	ctx, cancel := context.WithTimeout(context.Background(), time.Duration(flags.TIMEOUT)*time.Second)
	defer cancel()

	modelLoadRequest := RepositoryModelLoadRequest{ModelName: c.QueryParam("model")}
	modelLoadResponse, err := client.grpc.RepositoryModelLoad(ctx, &modelLoadRequest)
	if err != nil {
		return err
	}
	return c.JSON(http.StatusOK, modelLoadResponse)
}

// @Summary     Unload a model.
// @Description It requests to unload a model. This is only allowed when polling is enabled.
// @Accept      json
// @Produce     json
// @Param       model query    string                        true "model name"
// @Success     200   {object} RepositoryModelUnloadResponse "Triton server's model unload response"
// @Router      /model-unload [post]
func unloadModel(c echo.Context) error {
	ctx, cancel := context.WithTimeout(context.Background(), time.Duration(flags.TIMEOUT)*time.Second)
	defer cancel()

	modelUnloadRequest := RepositoryModelUnloadRequest{ModelName: c.QueryParam("model")}
	modelUnloadResponse, err := client.grpc.RepositoryModelUnload(ctx, &modelUnloadRequest)
	if err != nil {
		return err
	}
	return c.JSON(http.StatusOK, modelUnloadResponse)
}

// @Summary     Model inference api for the model with bytes a input and a bytes output.
// @Description It outputs a single bytes with a single bytes input.
// @Accept      json
// @Produce     json
// @Param       model   formData string             true  "model name"
// @Param       file    formData file               true  "input"
// @Param       version formData string             false "model version"
// @Success     200     {object} ModelInferResponse "Triton server's inference response"
// @Router      /infer [post]
func infer(c echo.Context) error {
	ctx, cancel := context.WithTimeout(context.Background(), time.Duration(flags.TIMEOUT)*time.Second)
	defer cancel()

	// Get the model information
	modelName := c.FormValue("model")
	modelVersion := c.FormValue("version")
	log.Println("1", modelName, modelVersion)

	// Get the file
	file, err := c.FormFile("file")
	if err != nil {
		return err
	}
	log.Println("2")
	fileContent, err := file.Open()
	if err != nil {
		return err
	}
	log.Println("3")
	rawInput, err := ioutil.ReadAll(fileContent)
	if err != nil {
		return err
	}
	log.Println("4")

	// Create request input / output tensors
	size := int64(len(rawInput))
	inferInputs := []*ModelInferRequest_InferInputTensor{{Name: "INPUT0", Datatype: "BYTES", Shape: []int64{size}}}
	inferOutputs := []*ModelInferRequest_InferRequestedOutputTensor{{Name: "OUTPUT0"}}
	log.Println("5", size)

	// Create a request
	modelInferRequest := ModelInferRequest{
		ModelName:        modelName,
		ModelVersion:     modelVersion,
		Inputs:           inferInputs,
		Outputs:          inferOutputs,
		RawInputContents: [][]byte{rawInput},
	}
	log.Println("6")

	// Get infer response
	modelInferResponse, err := client.grpc.ModelInfer(ctx, &modelInferRequest)
	if err != nil {
		return err
	}
	log.Println("7")
	return c.JSON(http.StatusOK, modelInferResponse)
}
